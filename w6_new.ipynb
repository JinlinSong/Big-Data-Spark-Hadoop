{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "import sys\n",
    "import re\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8') # required to convert to unicode\n",
    "stop_words = ['a','about','above','across','after','afterwards','again','against','all','almost','alone','along','already','also','although','always','am','among','amongst','amoungst','amount','an','and','another','any','anyhow','anyone','anything','anyway','anywhere','are','around','as','at','back','be','became','because','become','becomes','becoming','been','before','beforehand','behind','being','below','beside','besides','between','beyond','bill','both','bottom','but','by','call','can','cannot','cant','co','computer','con','could','couldnt','cry','de','describe','detail','do','done','down','due','during','each','eg','eight','either','eleven','else','elsewhere','empty','enough','etc','even','ever','every','everyone','everything','everywhere','except','few','fifteen','fify','fill','find','fire','first','five','for','former','formerly','forty','found','four','from','front','full','further','get','give','go','had','has','hasnt','have','he','hence','her','here','hereafter','hereby','herein','hereupon','hers','herse\\\"','him','himse\\\"','his','how','however','hundred','i','ie','if','in','inc','indeed','interest','into','is','it','its','itse\\\"','keep','last','latter','latterly','least','less','ltd','made','many','may','me','meanwhile','might','mill','mine','more','moreover','most','mostly','move','much','must','my','myse\\\"','name','namely','neither','never','nevertheless','next','nine','no','nobody','none','noone','nor','not','nothing','now','nowhere','of','off','often','on','once','one','only','onto','or','other','others','otherwise','our','ours','ourselves','out','over','own','part','per','perhaps','please','put','rather','re','same','see','seem','seemed','seeming','seems','serious','several','she','should','show','side','since','sincere','six','sixty','so','some','somehow','someone','something','sometime','sometimes','somewhere','still','such','system','take','ten','than','that','the','their','them','themselves','then','thence','there','thereafter','thereby','therefore','therein','thereupon','these','they','thick','thin','third','this','those','though','three','through','throughout','thru','thus','to','together','too','top','toward','towards','twelve','twenty','two','un','under','until','up','upon','us','very','via','was','we','well','were','what','whatever','when','whence','whenever','where','whereafter','whereas','whereby','wherein','whereupon','wherever','whether','which','while','whither','who','whoever','whole','whom','whose','why','will','with','within','without','would','yet','you','your','yours','yourself','yourselves']\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            key = word + \",\" + article_id\n",
    "            print \"%s\\t%d\" % (key, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "import sys\n",
    "current_article_id = None\n",
    "current_word = None\n",
    "word_sum = 0\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, count = line.strip().split('\\t', 1)\n",
    "        word, article_id = key.split(',', 1)\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    if current_article_id != article_id or current_word != word:\n",
    "        if current_article_id and current_word:\n",
    "            key = current_word + \",\" + current_article_id\n",
    "            print \"%s\\t%d\" % (key, word_sum)\n",
    "            word_sum = 0\n",
    "        current_article_id = article_id\n",
    "        current_word = word\n",
    "    word_sum += count\n",
    "key = current_word + \",\" + current_article_id\n",
    "print \"%s\\t%d\" % (key, word_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `tfidf_result_1522352174954688': No such file or directory\n",
      "18/03/29 19:36:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/03/29 19:36:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/03/29 19:36:18 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/03/29 19:36:18 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/03/29 19:36:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1522320992381_0004\n",
      "18/03/29 19:36:19 INFO impl.YarnClientImpl: Submitted application application_1522320992381_0004\n",
      "18/03/29 19:36:19 INFO mapreduce.Job: The url to track the job: http://d801e48a6a61:8088/proxy/application_1522320992381_0004/\n",
      "18/03/29 19:36:19 INFO mapreduce.Job: Running job: job_1522320992381_0004\n",
      "18/03/29 19:36:25 INFO mapreduce.Job: Job job_1522320992381_0004 running in uber mode : false\n",
      "18/03/29 19:36:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/03/29 19:36:41 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "18/03/29 19:36:47 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "18/03/29 19:36:53 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "18/03/29 19:36:59 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "18/03/29 19:37:05 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "18/03/29 19:37:11 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "18/03/29 19:37:16 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "18/03/29 19:37:22 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "18/03/29 19:37:28 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "18/03/29 19:37:34 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "18/03/29 19:37:40 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "18/03/29 19:37:46 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "18/03/29 19:37:52 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "18/03/29 19:37:58 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "18/03/29 19:38:05 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "18/03/29 19:38:11 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "18/03/29 19:38:17 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "18/03/29 19:38:23 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "18/03/29 19:38:29 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "18/03/29 19:38:35 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "18/03/29 19:38:41 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "18/03/29 19:38:47 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "18/03/29 19:38:53 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "18/03/29 19:38:57 INFO mapreduce.Job:  map 82% reduce 0%\n",
      "18/03/29 19:38:59 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "18/03/29 19:39:02 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/03/29 19:39:06 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "18/03/29 19:39:07 INFO mapreduce.Job:  map 100% reduce 38%\n",
      "18/03/29 19:39:08 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "18/03/29 19:39:09 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "18/03/29 19:39:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/03/29 19:39:13 INFO mapreduce.Job: Job job_1522320992381_0004 completed successfully\n",
      "18/03/29 19:39:13 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=232955961\n",
      "\t\tFILE: Number of bytes written=350754931\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=52516658\n",
      "\t\tHDFS: Number of read operations=30\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=8\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=306150\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=44203\n",
      "\t\tTotal time spent by all map tasks (ms)=306150\n",
      "\t\tTotal time spent by all reduce tasks (ms)=44203\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=306150\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=44203\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=313497600\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=45263872\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=6971026\n",
      "\t\tMap output bytes=102510699\n",
      "\t\tMap output materialized bytes=116452859\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3472288\n",
      "\t\tReduce shuffle bytes=116452859\n",
      "\t\tReduce input records=6971026\n",
      "\t\tReduce output records=3472288\n",
      "\t\tSpilled Records=20913078\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=942\n",
      "\t\tCPU time spent (ms)=350640\n",
      "\t\tPhysical memory (bytes) snapshot=2305536000\n",
      "\t\tVirtual memory (bytes) snapshot=20211916800\n",
      "\t\tTotal committed heap usage (bytes)=1605894144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=52516658\n",
      "18/03/29 19:39:13 INFO streaming.StreamJob: Output directory: tfidf_result_1522352174954688\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "NUM_REDUCERS=8\n",
    "OUT_DIR=\"tfidf_result_\"$(date +\"%s%6N\")\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Tfidf\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper \"python mapper.py\" \\\n",
    "    -reducer \"python reducer.py\" \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR} > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "-rw-r--r--   1 jovyan supergroup          0 2018-03-29 19:39 tfidf_result_1522352174954688/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan supergroup    6566868 2018-03-29 19:39 tfidf_result_1522352174954688/part-00000\r\n",
      "-rw-r--r--   1 jovyan supergroup    6569591 2018-03-29 19:39 tfidf_result_1522352174954688/part-00001\r\n",
      "-rw-r--r--   1 jovyan supergroup    6558042 2018-03-29 19:39 tfidf_result_1522352174954688/part-00002\r\n",
      "-rw-r--r--   1 jovyan supergroup    6562065 2018-03-29 19:39 tfidf_result_1522352174954688/part-00003\r\n",
      "-rw-r--r--   1 jovyan supergroup    6565405 2018-03-29 19:39 tfidf_result_1522352174954688/part-00004\r\n",
      "-rw-r--r--   1 jovyan supergroup    6561957 2018-03-29 19:39 tfidf_result_1522352174954688/part-00005\r\n",
      "-rw-r--r--   1 jovyan supergroup    6566523 2018-03-29 19:39 tfidf_result_1522352174954688/part-00006\r\n",
      "-rw-r--r--   1 jovyan supergroup    6566207 2018-03-29 19:39 tfidf_result_1522352174954688/part-00007\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls tfidf_result_1522352174954688/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%however,6970\t1\r\n",
      "0&\\mbox,3170\t1\r\n",
      "0(8)320-1231,2061\t1\r\n",
      "0)$and,4052\t1\r\n",
      "0,03,5188\t1\r\n",
      "0,1,0',3,2961\t1\r\n",
      "0,1,4664\t10\r\n",
      "0,1,5225\t1\r\n",
      "0,1,6042\t1\r\n",
      "0,1,\\dots,n,4594\t1\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00000|head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00000| grep \"^labor,\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00001| grep \"^labor,\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00002| grep \"^labor,\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00003| grep \"^labor,\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00004| grep \"^labor,\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00005| grep \"^labor,\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00006| grep \"^labor,\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00007| grep \"^labor,\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#274 totally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /data/wiki/en_articles_part/articles-part | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor,12\t12\r\n",
      "labor,2597\t12\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_result_1522352174954688/part-00006| grep \"^labor,.*12$\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper2.py\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, word_sum = line.split('\\t', 1)\n",
    "        word, article_id = key.split(',', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    value = word + \",\" + word_sum\n",
    "    print '%s\\t%s' % (article_id, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer2.py\n",
    "import sys\n",
    "article_word_count = 0\n",
    "current_article_id = None\n",
    "article_words = {}\n",
    "article_word_counts = {}\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, value = line.strip().split('\\t', 1)\n",
    "        word, word_sum = value.split(',', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "\n",
    "    if current_article_id != article_id:\n",
    "        if current_article_id:\n",
    "            article_word_counts[current_article_id] = article_word_count\n",
    "            article_word_count = 0\n",
    "        current_article_id = article_id\n",
    "    try:\n",
    "        article_word_count += int(word_sum)\n",
    "        if article_id in article_words.keys():\n",
    "            temp_list = article_words[article_id]\n",
    "            temp_list.append(word + \"\\t\" + word_sum)\n",
    "            article_words[article_id] = temp_list\n",
    "        else:\n",
    "            temp_list = []\n",
    "            temp_list.append(word + \"\\t\" + word_sum) \n",
    "            article_words[article_id] = temp_list\n",
    "    except:\n",
    "        continue\n",
    "article_word_counts[current_article_id] = article_word_count\n",
    "for key in article_words.keys():\n",
    "    items = article_words[key]\n",
    "    for item in items:\n",
    "        word, word_sum = item.strip().split(\"\\t\", 1)\n",
    "        _key = word + \",\" + key\n",
    "        _value = word_sum + \",\" + str(article_word_counts[key])\n",
    "        print '%s\\t%s' % (_key, _value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `tfidf_interim_result_1522347067568010': No such file or directory\n",
      "18/03/29 18:11:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/03/29 18:11:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/03/29 18:11:11 INFO mapred.FileInputFormat: Total input files to process : 8\n",
      "18/03/29 18:11:11 INFO mapreduce.JobSubmitter: number of splits:8\n",
      "18/03/29 18:11:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1522320992381_0002\n",
      "18/03/29 18:11:11 INFO impl.YarnClientImpl: Submitted application application_1522320992381_0002\n",
      "18/03/29 18:11:11 INFO mapreduce.Job: The url to track the job: http://d801e48a6a61:8088/proxy/application_1522320992381_0002/\n",
      "18/03/29 18:11:11 INFO mapreduce.Job: Running job: job_1522320992381_0002\n",
      "18/03/29 18:11:17 INFO mapreduce.Job: Job job_1522320992381_0002 running in uber mode : false\n",
      "18/03/29 18:11:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/03/29 18:11:24 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/03/29 18:11:25 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "18/03/29 18:11:30 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/03/29 18:11:41 INFO mapreduce.Job:  map 100% reduce 12%\n",
      "18/03/29 18:11:42 INFO mapreduce.Job:  map 100% reduce 24%\n",
      "18/03/29 18:11:43 INFO mapreduce.Job:  map 100% reduce 49%\n",
      "18/03/29 18:11:45 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "18/03/29 18:11:48 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "18/03/29 18:11:58 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/03/29 18:11:59 INFO mapreduce.Job: Job job_1522320992381_0002 completed successfully\n",
      "18/03/29 18:11:59 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=73350457\n",
      "\t\tFILE: Number of bytes written=148936938\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=52517666\n",
      "\t\tHDFS: Number of bytes written=69295868\n",
      "\t\tHDFS: Number of read operations=48\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=9\n",
      "\t\tData-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35155\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=129548\n",
      "\t\tTotal time spent by all map tasks (ms)=35155\n",
      "\t\tTotal time spent by all reduce tasks (ms)=129548\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=35155\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=129548\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35998720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=132657152\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3472288\n",
      "\t\tMap output records=6944576\n",
      "\t\tMap output bytes=59461245\n",
      "\t\tMap output materialized bytes=73350793\n",
      "\t\tInput split bytes=1008\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=18237\n",
      "\t\tReduce shuffle bytes=73350793\n",
      "\t\tReduce input records=6944576\n",
      "\t\tReduce output records=3472288\n",
      "\t\tSpilled Records=13889152\n",
      "\t\tShuffled Maps =64\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=64\n",
      "\t\tGC time elapsed (ms)=1644\n",
      "\t\tCPU time spent (ms)=94290\n",
      "\t\tPhysical memory (bytes) snapshot=4108836864\n",
      "\t\tVirtual memory (bytes) snapshot=32287162368\n",
      "\t\tTotal committed heap usage (bytes)=2816475136\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=52516658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=69295868\n",
      "18/03/29 18:11:59 INFO streaming.StreamJob: Output directory: tfidf_interim_result_1522347067568010\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "NUM_REDUCERS=8\n",
    "OUT_INTERIM_DIR=\"tfidf_interim_result_\"$(date +\"%s%6N\")\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_INTERIM_DIR} > /dev/null\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Tfidf\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper2.py,reducer2.py \\\n",
    "    -mapper \"python mapper2.py\" \\\n",
    "    -reducer \"python reducer2.py\" \\\n",
    "    -input tfidf_result_1522352174954688 \\\n",
    "    -output ${OUT_INTERIM_DIR} > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "-rw-r--r--   1 jovyan supergroup          0 2018-03-29 18:11 tfidf_interim_result_1522347067568010/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan supergroup    8634905 2018-03-29 18:11 tfidf_interim_result_1522347067568010/part-00000\r\n",
      "-rw-r--r--   1 jovyan supergroup    8125858 2018-03-29 18:11 tfidf_interim_result_1522347067568010/part-00001\r\n",
      "-rw-r--r--   1 jovyan supergroup    8501952 2018-03-29 18:11 tfidf_interim_result_1522347067568010/part-00002\r\n",
      "-rw-r--r--   1 jovyan supergroup    8561766 2018-03-29 18:11 tfidf_interim_result_1522347067568010/part-00003\r\n",
      "-rw-r--r--   1 jovyan supergroup    8613223 2018-03-29 18:11 tfidf_interim_result_1522347067568010/part-00004\r\n",
      "-rw-r--r--   1 jovyan supergroup    8439894 2018-03-29 18:11 tfidf_interim_result_1522347067568010/part-00005\r\n",
      "-rw-r--r--   1 jovyan supergroup    9553554 2018-03-29 18:11 tfidf_interim_result_1522347067568010/part-00006\r\n",
      "-rw-r--r--   1 jovyan supergroup    8864716 2018-03-29 18:11 tfidf_interim_result_1522347067568010/part-00007\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls tfidf_interim_result_1522347067568010/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor,12\t12,6092\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_interim_result_1522347067568010/part-00002| grep \"^labor,12\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor,1254\t2,2906\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_interim_result_1522347067568010/part-00001| grep \"^labor,1254\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor,12\t12,6092\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_interim_result_1522347067568010/part-00002| grep \"^labor,.*12,6092$\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.py\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split('\\t', 1)\n",
    "    word, article_id = key.split(',', 1)\n",
    "    word_sum, words_in_article = value.split(',', 1)\n",
    "    value = article_id + \",\" + word_sum + \",\" + words_in_article + \",\" + str(1)\n",
    "    print '%s\\t%s' % (word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.py\n",
    "import sys\n",
    "import math\n",
    "current_word = None\n",
    "word_in_articles_count = 0\n",
    "word_stats_dict = {}\n",
    "word_in_articles_counts = {}\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "      word, value = line.strip().split('\\t', 1)\n",
    "      word_stats = value.split(',')\n",
    "      count = int(word_stats[3])\n",
    "    except ValueError as e:\n",
    "      continue\n",
    "\n",
    "    if current_word != word:\n",
    "        if current_word:\n",
    "            word_in_articles_counts[current_word] = word_in_articles_count\n",
    "            word_in_articles_count = 0\n",
    "        current_word = word\n",
    "    word_in_articles_count += int(count)\n",
    "    \n",
    "    if word in word_stats_dict.keys():\n",
    "        word_stats_dict[word].append(word_stats[0] + \",\" + word_stats[1] + \",\" + word_stats[2])\n",
    "    else:\n",
    "        word_stats_dict[word] = []\n",
    "        word_stats_dict[word].append(word_stats[0] + \",\" + word_stats[1] + \",\" + word_stats[2])\n",
    "word_in_articles_counts[current_word] = word_in_articles_count\n",
    "for word in word_in_articles_counts.keys():\n",
    "    word_stats_list = word_stats_dict[word]\n",
    "    for word_stats in word_stats_list:\n",
    "        word_stats = word_stats.split(\",\")\n",
    "        try :\n",
    "          key = word + \",\" + word_stats[0]\n",
    "          print \"%s\\t%s\\t%s\\t%s\" % (key, word_stats[1], word_stats[2], word_in_articles_counts[word])\n",
    "        except ZeroDivisionError as e:\n",
    "          continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `tfidf_final_result_1522348436765036': No such file or directory\n",
      "18/03/29 18:34:00 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/03/29 18:34:00 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/03/29 18:34:00 INFO mapred.FileInputFormat: Total input files to process : 8\n",
      "18/03/29 18:34:00 INFO mapreduce.JobSubmitter: number of splits:8\n",
      "18/03/29 18:34:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1522320992381_0003\n",
      "18/03/29 18:34:01 INFO impl.YarnClientImpl: Submitted application application_1522320992381_0003\n",
      "18/03/29 18:34:01 INFO mapreduce.Job: The url to track the job: http://d801e48a6a61:8088/proxy/application_1522320992381_0003/\n",
      "18/03/29 18:34:01 INFO mapreduce.Job: Running job: job_1522320992381_0003\n",
      "18/03/29 18:34:06 INFO mapreduce.Job: Job job_1522320992381_0003 running in uber mode : false\n",
      "18/03/29 18:34:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/03/29 18:34:13 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "18/03/29 18:34:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/03/29 18:34:29 INFO mapreduce.Job:  map 100% reduce 9%\n",
      "18/03/29 18:34:30 INFO mapreduce.Job:  map 100% reduce 18%\n",
      "18/03/29 18:34:31 INFO mapreduce.Job:  map 100% reduce 27%\n",
      "18/03/29 18:34:32 INFO mapreduce.Job:  map 100% reduce 36%\n",
      "18/03/29 18:34:33 INFO mapreduce.Job:  map 100% reduce 55%\n",
      "18/03/29 18:34:39 INFO mapreduce.Job:  map 100% reduce 56%\n",
      "18/03/29 18:34:45 INFO mapreduce.Job:  map 100% reduce 57%\n",
      "18/03/29 18:34:51 INFO mapreduce.Job:  map 100% reduce 58%\n",
      "18/03/29 18:35:00 INFO mapreduce.Job:  map 100% reduce 59%\n",
      "18/03/29 18:35:08 INFO mapreduce.Job:  map 100% reduce 60%\n",
      "18/03/29 18:35:19 INFO mapreduce.Job:  map 100% reduce 61%\n",
      "18/03/29 18:35:35 INFO mapreduce.Job:  map 100% reduce 62%\n",
      "18/03/29 18:35:53 INFO mapreduce.Job:  map 100% reduce 63%\n",
      "18/03/29 18:36:19 INFO mapreduce.Job:  map 100% reduce 64%\n",
      "18/03/29 18:36:46 INFO mapreduce.Job:  map 100% reduce 65%\n",
      "18/03/29 18:37:15 INFO mapreduce.Job:  map 100% reduce 66%\n",
      "18/03/29 18:37:45 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "18/03/29 18:38:21 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "18/03/29 18:39:03 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "18/03/29 18:39:43 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "18/03/29 18:40:26 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "18/03/29 18:41:14 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "18/03/29 18:42:04 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "18/03/29 18:43:01 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "18/03/29 18:44:03 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "18/03/29 18:44:23 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "18/03/29 18:44:43 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "18/03/29 18:45:01 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "18/03/29 18:45:35 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "18/03/29 18:46:43 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "18/03/29 18:48:18 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "18/03/29 18:50:08 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "18/03/29 18:52:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/03/29 18:54:42 INFO mapreduce.Job: Job job_1522320992381_0003 completed successfully\n",
      "18/03/29 18:54:43 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83185088\n",
      "\t\tFILE: Number of bytes written=168606296\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=69296940\n",
      "\t\tHDFS: Number of bytes written=81606846\n",
      "\t\tHDFS: Number of read operations=48\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=9\n",
      "\t\tData-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35155\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5431300\n",
      "\t\tTotal time spent by all map tasks (ms)=35155\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5431300\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=35155\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5431300\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35998720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5561651200\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3472288\n",
      "\t\tMap output records=3472288\n",
      "\t\tMap output bytes=76240454\n",
      "\t\tMap output materialized bytes=83185424\n",
      "\t\tInput split bytes=1072\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=415037\n",
      "\t\tReduce shuffle bytes=83185424\n",
      "\t\tReduce input records=3472288\n",
      "\t\tReduce output records=3472234\n",
      "\t\tSpilled Records=6944576\n",
      "\t\tShuffled Maps =64\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=64\n",
      "\t\tGC time elapsed (ms)=1668\n",
      "\t\tCPU time spent (ms)=4860340\n",
      "\t\tPhysical memory (bytes) snapshot=4045811712\n",
      "\t\tVirtual memory (bytes) snapshot=32263278592\n",
      "\t\tTotal committed heap usage (bytes)=2747793408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=69295868\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=81606846\n",
      "18/03/29 18:54:43 INFO streaming.StreamJob: Output directory: tfidf_final_result_1522348436765036\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "NUM_REDUCERS=8\n",
    "OUT_FINAL_DIR=\"tfidf_final_result_\"$(date +\"%s%6N\")\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_FINAL_DIR} > /dev/null\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Tfidf\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper3.py,reducer3.py \\\n",
    "    -mapper \"python mapper3.py\" \\\n",
    "    -reducer \"python reducer3.py\" \\\n",
    "    -input tfidf_interim_result_1522347067568010 \\\n",
    "    -output ${OUT_FINAL_DIR} > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "-rw-r--r--   1 jovyan supergroup          0 2018-03-29 18:54 tfidf_final_result_1522348436765036/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan supergroup   10549246 2018-03-29 18:44 tfidf_final_result_1522348436765036/part-00000\r\n",
      "-rw-r--r--   1 jovyan supergroup   10160181 2018-03-29 18:44 tfidf_final_result_1522348436765036/part-00001\r\n",
      "-rw-r--r--   1 jovyan supergroup   10343749 2018-03-29 18:44 tfidf_final_result_1522348436765036/part-00002\r\n",
      "-rw-r--r--   1 jovyan supergroup   10203016 2018-03-29 18:44 tfidf_final_result_1522348436765036/part-00003\r\n",
      "-rw-r--r--   1 jovyan supergroup    9883640 2018-03-29 18:44 tfidf_final_result_1522348436765036/part-00004\r\n",
      "-rw-r--r--   1 jovyan supergroup   10389818 2018-03-29 18:45 tfidf_final_result_1522348436765036/part-00005\r\n",
      "-rw-r--r--   1 jovyan supergroup   10316351 2018-03-29 18:54 tfidf_final_result_1522348436765036/part-00006\r\n",
      "-rw-r--r--   1 jovyan supergroup    9760845 2018-03-29 18:53 tfidf_final_result_1522348436765036/part-00007\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls tfidf_final_result_1522348436765036/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor,12\t12\t6092\t274\r\n",
      "labor,1254\t2\t2906\t274\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_final_result_1522348436765036/part-00007| grep \"^labor,12\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor,12\t12\t6092\t274\r\n",
      "labor,1254\t2\t2906\t274\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tfidf_final_result_1522348436765036/part-00007| grep \"^labor,12.*274$\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float(12/(126092*math.log10(1+275))) = 3.898900116685244e-05\n",
    "# (12/6092) * (1/math.log(1+274)) = 0.00035069907961617183\n",
    "# should be 0.000350696420907907\n",
    "# 274 should be 275 ?\n",
    "# 3.898900116685244e-05\n",
    "# 4100 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NUM_REDUCERS=8\n",
    "\n",
    "OUT_DIR=\"tfidf_result_\"$(date +\"%s%6N\")\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Tfidf\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper \"python mapper.py\" \\\n",
    "    -reducer \"python reducer.py\" \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR} > /dev/null\n",
    "    \n",
    "OUT_INTERIM_DIR=\"tfidf_interim_result_\"$(date +\"%s%6N\")\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_INTERIM_DIR} > /dev/null\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Tfidf\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper2.py,reducer2.py \\\n",
    "    -mapper \"python mapper2.py\" \\\n",
    "    -reducer \"python reducer2.py\" \\\n",
    "    -input ${OUT_DIR} \\\n",
    "    -output ${OUT_INTERIM_DIR} > /dev/null\n",
    "    \n",
    "OUT_FINAL_DIR=\"tfidf_final_result_\"$(date +\"%s%6N\")\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_FINAL_DIR} > /dev/null\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Tfidf\" \\\n",
    "    -D mapreduce.job.reduces=8 \\\n",
    "    -files mapper3.py,reducer3.py \\\n",
    "    -mapper \"python mapper3.py\" \\\n",
    "    -reducer \"python reducer3.py\" \\\n",
    "    -input ${OUT_INTERIM_DIR} \\\n",
    "    -output ${OUT_FINAL_DIR} > /dev/null 2>${OUT_FINAL_DIR}\n",
    "\n",
    "cat ${OUT_FINAL_DIR} >&2\n",
    "grep 'labor,12' ${OUT_LOG} \\\n",
    "    | cut -d'=' -f2 \\\n",
    "    | paste -sd' ' \\\n",
    "    | awk '{ printf \"%.2f\", ($1 / $2 * 100) }'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
