{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "import sys\n",
    "import re\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8') # required to convert to unicode\n",
    "stop_words = ['a','about','above','across','after','afterwards','again','against','all','almost','alone','along','already','also','although','always','am','among','amongst','amoungst','amount','an','and','another','any','anyhow','anyone','anything','anyway','anywhere','are','around','as','at','back','be','became','because','become','becomes','becoming','been','before','beforehand','behind','being','below','beside','besides','between','beyond','bill','both','bottom','but','by','call','can','cannot','cant','co','computer','con','could','couldnt','cry','de','describe','detail','do','done','down','due','during','each','eg','eight','either','eleven','else','elsewhere','empty','enough','etc','even','ever','every','everyone','everything','everywhere','except','few','fifteen','fify','fill','find','fire','first','five','for','former','formerly','forty','found','four','from','front','full','further','get','give','go','had','has','hasnt','have','he','hence','her','here','hereafter','hereby','herein','hereupon','hers','herse\\\"','him','himse\\\"','his','how','however','hundred','i','ie','if','in','inc','indeed','interest','into','is','it','its','itse\\\"','keep','last','latter','latterly','least','less','ltd','made','many','may','me','meanwhile','might','mill','mine','more','moreover','most','mostly','move','much','must','my','myse\\\"','name','namely','neither','never','nevertheless','next','nine','no','nobody','none','noone','nor','not','nothing','now','nowhere','of','off','often','on','once','one','only','onto','or','other','others','otherwise','our','ours','ourselves','out','over','own','part','per','perhaps','please','put','rather','re','same','see','seem','seemed','seeming','seems','serious','several','she','should','show','side','since','sincere','six','sixty','so','some','somehow','someone','something','sometime','sometimes','somewhere','still','such','system','take','ten','than','that','the','their','them','themselves','then','thence','there','thereafter','thereby','therefore','therein','thereupon','these','they','thick','thin','third','this','those','though','three','through','throughout','thru','thus','to','together','too','top','toward','towards','twelve','twenty','two','un','under','until','up','upon','us','very','via','was','we','well','were','what','whatever','when','whence','whenever','where','whereafter','whereas','whereby','wherein','whereupon','wherever','whether','which','while','whither','who','whoever','whole','whom','whose','why','will','with','within','without','would','yet','you','your','yours','yourself','yourselves']\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and (word == 'labor' or int(article_id) == 12):\n",
    "            key = word + \",\" + article_id\n",
    "            print \"%s\\t%d\" % (key, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "import sys\n",
    "current_article_id = None\n",
    "current_word = None\n",
    "word_sum = 0\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, count = line.strip().split('\\t', 1)\n",
    "        word, article_id = key.split(',', 1)\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    if current_article_id != article_id or current_word != word:\n",
    "        if current_article_id and current_word:\n",
    "            key = current_word + \",\" + current_article_id\n",
    "            _value = str(word_sum) + \",\" + str(6092)\n",
    "            print \"%s\\t%s\" % (key, _value)\n",
    "            word_sum = 0\n",
    "        current_article_id = article_id\n",
    "        current_word = word\n",
    "    word_sum += count\n",
    "key = current_word + \",\" + current_article_id\n",
    "_value = str(word_sum) + \",\" + str(6092)\n",
    "print \"%s\\t%s\" % (key, _value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.py\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    key, value = line.strip().split('\\t', 1)\n",
    "    word, article_id = key.split(',', 1)\n",
    "    word_sum, words_in_article = value.split(',', 1)\n",
    "    value = article_id + \",\" + word_sum + \",\" + words_in_article + \",\" + str(1)\n",
    "    print '%s\\t%s' % (word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.py\n",
    "import sys\n",
    "import math\n",
    "current_word = None\n",
    "word_in_articles_count = 0\n",
    "word_stats_dict = {}\n",
    "word_in_articles_counts = {}\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "      word, value = line.strip().split('\\t', 1)\n",
    "      word_stats = value.split(',')\n",
    "      count = int(word_stats[3])\n",
    "    except ValueError as e:\n",
    "      continue\n",
    "    if current_word != word:\n",
    "        if current_word:\n",
    "            word_in_articles_counts[current_word] = word_in_articles_count\n",
    "            word_in_articles_count = 0\n",
    "        current_word = word\n",
    "    word_in_articles_count += int(count)\n",
    "    \n",
    "    if word in word_stats_dict.keys():\n",
    "        word_stats_dict[word].append(word_stats[0] + \",\" + word_stats[1] + \",\" + word_stats[2])\n",
    "    else:\n",
    "        word_stats_dict[word] = []\n",
    "        word_stats_dict[word].append(word_stats[0] + \",\" + word_stats[1] + \",\" + word_stats[2])\n",
    "word_in_articles_counts[current_word] = word_in_articles_count\n",
    "for word in word_in_articles_counts.keys():\n",
    "    word_stats_list = word_stats_dict[word]\n",
    "    for word_stats in word_stats_list:\n",
    "        word_stats = word_stats.split(\",\")\n",
    "        try :\n",
    "          key = word + \",\" + word_stats[0]\n",
    "          print \"%s\\t%f\" % (key, (float(word_stats[1])/float(word_stats[2]))*(1/math.log(1+word_in_articles_counts[word])))  \n",
    "        except ZeroDivisionError as e:\n",
    "          continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NUM_REDUCERS=8\n",
    "OUT_DIR=\"tfidf_result_\"$(date +\"%s%6N\")\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Tfidf\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper \"python mapper.py\" \\\n",
    "    -reducer \"python reducer.py\" \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR} > /dev/null\n",
    "\n",
    "OUT_FINAL_DIR=\"tfidf_final_result_\"$(date +\"%s%6N\")\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_FINAL_DIR} > /dev/null\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Tfidf\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -files mapper3.py,reducer3.py \\\n",
    "    -mapper \"python mapper3.py\" \\\n",
    "    -reducer \"python reducer3.py\" \\\n",
    "    -input ${OUT_DIR} \\\n",
    "    -output ${OUT_FINAL_DIR} > /dev/null\n",
    "hdfs dfs -cat ${OUT_FINAL_DIR}/part-00000 | awk -F '\\t' '$1 == \"labor,12\" {print $2}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
